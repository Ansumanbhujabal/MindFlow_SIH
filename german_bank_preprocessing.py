# -*- coding: utf-8 -*-
"""German_Bank_Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14oMGwNwqx8NtNAkoV77ClF7KtQLSEAcS

# **IMPORT**
"""

from imblearn.over_sampling import ADASYN
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import warnings
import pickle

data=pd.read_csv("/content/bapu space text.csv")
# data.drop(['Unnamed: 0'],axis=1,inplace=True)

data.head()

columns = ['status', 'duration', 'credit_history', 'purpose', 'amount', 'savings', 'employment_duration',
           'installment_rate', 'personal_status_sex', 'other_debtors', 'present_residence', 'property',
           'age', 'other_installment_plans', 'housing', 'number_credits', 'job', 'people_liable', 'telephone',
           'foreign_worker', 'credit_risk']

data.columns=columns

data.head()

q1 = data.quantile(0.25)
q3 = data.quantile(0.75)
IQR = q3 - q1

((data < (q1 - 1.5 * IQR)) | (data > (q3 + 1.5 * IQR))).sum()



for i in columns:
    q75, q25 = np.percentile(data[i], [75, 25])
    iqr = q75 - q25
    minimum = q25 - 1.5 * iqr
    maximum = q75 + 1.5 * iqr
    data.loc[data[i] < minimum, i] = minimum
    data.loc[data[i] > maximum, i] = maximum

data.head()

outliers_count = ((data < (q1 - 1.5 * IQR)) | (data > (q3 + 1.5 * IQR))).sum()

data_skew = data.skew()

cols_to_log_transform = ['amount', 'savings', 'number_credits']
for i in cols_to_log_transform:
    data[i] = np.log(data[i] + 1)

skewed_data = data[cols_to_log_transform].skew()

for i in data.columns:
    sns.displot(data=data, x=i, kde=True)

del data['other_debtors']
del data['other_installment_plans']
del data['housing']
del data['job']
del data['people_liable']
del data['foreign_worker']

features_for_scaling = [feature for feature in data.columns if feature not in ['credit_risk']]

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(data[features_for_scaling])
scaled_data = scaler.transform(data[features_for_scaling])
df2 = pd.DataFrame(scaled_data, columns=data[features_for_scaling].columns)

final = pd.concat([data[['credit_risk']].reset_index(drop=True), df2], axis=1)
final.head()

final.to_csv(r'SouthGerman_Preprocessed.csv')

file = 'Scaler_Credit_Data.pkl'
pickle.dump(scaler, open(file, 'wb'))

